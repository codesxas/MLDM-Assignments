{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLDM #9",
      "provenance": [],
      "authorship_tag": "ABX9TyN55GH5r7+6n6RuBoVirq7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codesxas/MLDM-Assignments/blob/master/Assignment_9/MLDM_lab_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsCwcW6tmg-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtZRpTkCTy9E",
        "colab_type": "text"
      },
      "source": [
        "Problem 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfy1KnC8mhH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadData (file_path):\n",
        "  df = pd.read_csv(file_path)\n",
        "  y = df['5'].to_numpy()\n",
        "  df.drop(columns='5', inplace=True)\n",
        "  x = df.to_numpy()\n",
        "\n",
        "  return x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrKwXlirq-pQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "38f32a37-a058-4f39-877d-1fca5f15cd21"
      },
      "source": [
        "x, y = loadData('mnist_train.csv')\n",
        "print(y.shape)\n",
        "print(x.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9999,)\n",
            "(9999, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yFhIm4XmhKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.25,random_state=42)\n",
        "\n",
        "Labels = pd.get_dummies(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTsl6GxLsQ8Q",
        "colab_type": "text"
      },
      "source": [
        "Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VwyDnTDXCCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron():\n",
        "    def __init__(self,x,y):\n",
        "        self.x = x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y = y\n",
        "        self.weights = []\n",
        "        self.bias = []\n",
        "        self.outputs = []\n",
        "        self.derivatives = []\n",
        "        self.activations = []\n",
        "        \n",
        "    def connect(self, layer1, layer2):\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "\n",
        "\n",
        "    def softmax(self, z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e, axis=1).reshape(-1, 1) \n",
        "    \n",
        "    def max_log_likelihood(self, y_pred, y):\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self, y, y_pred):\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self, x, y, weights, bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples = len(x)\n",
        "            ones_array = np.ones(samples).reshape(samples, 1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z = np.dot(np.append(ones_array, x, axis=1),weights[i]+bias[i])\n",
        "            x = self.softmax(z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred = x\n",
        "        temp = -self.max_log_likelihood(self.y_pred, y)\n",
        "        cost = np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array = np.ones(len(neuron.outputs[i])).reshape(len(neuron.outputs[i]), 1)\n",
        "            prev_term = self.delta_mll(y, self.y_pred)  \n",
        "            # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            x=self.softmax(z) \n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0H9JTVGmhMs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "5e9417ad-16a7-4ba1-cb99-ef419f279d49"
      },
      "source": [
        "neuron = Perceptron(x_train,Labels)\n",
        "neuron.connect(x_train, Labels)\n",
        "neuron.train(batches=1000, lr=0.2, epoch=30)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.001576014225144764\n",
            "1 0.0011333198187482582\n",
            "2 0.0012372264078219714\n",
            "3 0.0013989832011237495\n",
            "4 0.0014262177775442157\n",
            "5 0.0012822252459975473\n",
            "6 0.0012285712114221256\n",
            "7 0.001209359039257535\n",
            "8 0.0011668642019784412\n",
            "9 0.0011518753435283144\n",
            "10 0.0010590585679035832\n",
            "11 0.0010239267975151115\n",
            "12 0.000966569459911783\n",
            "13 0.0008983535075969271\n",
            "14 0.0008224324622228703\n",
            "15 0.0007494184830235992\n",
            "16 0.0006823447543905542\n",
            "17 0.0006162499323402363\n",
            "18 0.0005459317630848759\n",
            "19 0.0004736235437849922\n",
            "20 0.00040457737580726897\n",
            "21 0.00034076747001493166\n",
            "22 0.0002831053786427962\n",
            "23 0.00023317215338180478\n",
            "24 0.00019309581231568578\n",
            "25 0.00016456854945019248\n",
            "26 0.0001464674092700145\n",
            "27 0.00013591192028107992\n",
            "28 0.00012991245106966186\n",
            "29 0.00012578340787275168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhmGNmTfmhPA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e860c121-740c-49e8-8048-d784964020ed"
      },
      "source": [
        "pred = neuron.predict(x_test)\n",
        "np.bincount(neuron.predict(x_test)),np.bincount(y_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([267, 278, 265, 277, 272, 242, 236, 264, 160, 239]),\n",
              " array([256, 268, 262, 271, 266, 232, 231, 276, 206, 232]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnmMxm1pmhRT",
        "colab_type": "code",
        "outputId": "086f7435-b455-4601-e199-42f2b968c106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 88.92 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bET681h3tatK",
        "colab_type": "text"
      },
      "source": [
        "Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZSv2SVOmhT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class SingleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(self.outputs[i])).reshape(len(self.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi-dpWIQZ_N4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "bcd10f09-8937-41a1-a09f-f8efa346a119"
      },
      "source": [
        "neuron = SingleLayerNeuralNetwork(x_train, Labels)\n",
        "layer = Layer(100)\n",
        "neuron.connect(x_train, layer)\n",
        "neuron.connect(layer, Labels)\n",
        "neuron.train(batches=1000,lr=0.1,epoch=50)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.00028759624659275326\n",
            "1 0.00032420847030328426\n",
            "2 0.00039098816005103367\n",
            "3 0.00042494329662581457\n",
            "4 0.0004098569547816444\n",
            "5 0.0003617075379566328\n",
            "6 0.00029738642332423565\n",
            "7 0.00024350285311956275\n",
            "8 0.00019415866781650375\n",
            "9 0.00015385196003813996\n",
            "10 0.00012303890905348316\n",
            "11 9.814271092557557e-05\n",
            "12 7.845237405599915e-05\n",
            "13 6.400396913720192e-05\n",
            "14 5.3491145206895864e-05\n",
            "15 4.5471626140973745e-05\n",
            "16 3.9121583383108105e-05\n",
            "17 3.3973941353526116e-05\n",
            "18 2.973257336724531e-05\n",
            "19 2.6217511571827444e-05\n",
            "20 2.3312583101697508e-05\n",
            "21 2.0929133659263635e-05\n",
            "22 1.8986838101740678e-05\n",
            "23 1.7407047292848282e-05\n",
            "24 1.610956433915365e-05\n",
            "25 1.5016055043572032e-05\n",
            "26 1.4069854425211783e-05\n",
            "27 1.3245747763101272e-05\n",
            "28 1.2533027613417771e-05\n",
            "29 1.1910148658985198e-05\n",
            "30 1.1356024744385212e-05\n",
            "31 1.0855711284144597e-05\n",
            "32 1.039866976409461e-05\n",
            "33 9.977346674632008e-06\n",
            "34 9.586272105021964e-06\n",
            "35 9.221406555216508e-06\n",
            "36 8.87967951646072e-06\n",
            "37 8.55868166278962e-06\n",
            "38 8.256465015571151e-06\n",
            "39 7.971412997550301e-06\n",
            "40 7.702154307928075e-06\n",
            "41 7.447504266984995e-06\n",
            "42 7.206423685417427e-06\n",
            "43 6.977989216484694e-06\n",
            "44 6.761371464033293e-06\n",
            "45 6.55581849080439e-06\n",
            "46 6.360643193642332e-06\n",
            "47 6.17521351516677e-06\n",
            "48 5.9989447771008795e-06\n",
            "49 5.831293624581687e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVFHkiC3aDb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1cba1a49-eabe-42aa-ec15-9e6f8b34d670"
      },
      "source": [
        "pred=neuron.predict(x_test)\n",
        "np.bincount(neuron.predict(x_test)),np.bincount(y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([256, 268, 262, 273, 260, 227, 238, 275, 205, 236]),\n",
              " array([256, 268, 262, 271, 266, 232, 231, 276, 206, 232]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR_V0CGqaGuk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "320face7-bd45-4915-f909-20941f2a2f6b"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 92.4 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eKqtiuuy4sU",
        "colab_type": "text"
      },
      "source": [
        "Problem 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL7MozNfmhWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class DoubleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(self.outputs[i])).reshape(len(self.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jV4c6dFbUa7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "84b8c7c2-26d3-4882-da6f-ae941ddef411"
      },
      "source": [
        "neuron = DoubleLayerNeuralNetwork(x_train, Labels)\n",
        "layer1 = Layer(100)\n",
        "layer2 = Layer(100)\n",
        "neuron.connect(x_train, layer1)\n",
        "neuron.connect(layer1, layer2)\n",
        "neuron.connect(layer2, Labels)\n",
        "neuron.train(batches=1000, lr=0.1, epoch=20)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0009246753650025163\n",
            "1 0.0006890180770856735\n",
            "2 0.0007660625429046436\n",
            "3 0.0007974766193253802\n",
            "4 0.0007940729326666774\n",
            "5 0.0007863715120282105\n",
            "6 0.0007627120251324064\n",
            "7 0.0007231438994891776\n",
            "8 0.0006670903351895808\n",
            "9 0.00060296516834619\n",
            "10 0.0005379537057706004\n",
            "11 0.00046747929709260944\n",
            "12 0.00038790942945181545\n",
            "13 0.00030298736969095613\n",
            "14 0.00022530915977293195\n",
            "15 0.00016738294062619213\n",
            "16 0.00013021973476943314\n",
            "17 0.00010630107279707621\n",
            "18 8.952215006901322e-05\n",
            "19 7.670710813667534e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kML__CkbUPw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "71760e9e-1672-4ee0-8a00-8e8139b413ce"
      },
      "source": [
        "pred = neuron.predict(x_test)\n",
        "np.bincount(neuron.predict(x_test)),np.bincount(y_test)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([269, 264, 261, 265, 269, 211, 248, 277, 198, 238]),\n",
              " array([256, 268, 262, 271, 266, 232, 231, 276, 206, 232]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbYUpIKFcghi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c0b08c7-a2af-46f4-c818-42b2330de5ae"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 90.64 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuIMNxuHyoIm",
        "colab_type": "text"
      },
      "source": [
        "Problem 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip0At2PHw6pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkActivations():\n",
        "    def __init__(self,x,y):\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(self.outputs[i])).reshape(len(self.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezTmeT5qyaY0",
        "colab_type": "code",
        "outputId": "5eeb24ca-b8e7-450e-e431-1d2a94d0dad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "neuron=NeuralNetworkActivations(x_train,Labels)\n",
        "layer1 = Layer(100, 'sigmoid')\n",
        "layer2 = Layer(50, 'tanh')\n",
        "neuron.connect(x_train, layer1)\n",
        "neuron.connect(layer1, layer2)\n",
        "neuron.connect(layer2, Labels)\n",
        "neuron.train(batches=1000, lr=0.1, epoch=20)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.00022902306486852664\n",
            "1 0.00016994320112346742\n",
            "2 9.380919530023971e-05\n",
            "3 0.00028509603182765887\n",
            "4 0.0001983337955970099\n",
            "5 7.080909251828251e-05\n",
            "6 0.00041963409453966614\n",
            "7 3.795911883176497e-05\n",
            "8 3.0138572660946493e-05\n",
            "9 1.35979999587421e-05\n",
            "10 2.1028787768211475e-05\n",
            "11 9.358667349194094e-05\n",
            "12 6.399837832880523e-06\n",
            "13 5.412895269344516e-06\n",
            "14 1.5795681504797766e-06\n",
            "15 1.1282692025572214e-05\n",
            "16 0.00011007114581353774\n",
            "17 1.0964291650900242e-06\n",
            "18 2.562533901532028e-06\n",
            "19 2.860794166002153e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvP0-90JczQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c13971b3-5783-45fb-fd50-42090ec9c689"
      },
      "source": [
        "pred = neuron.predict(x_test)\n",
        "np.bincount(neuron.predict(x_test)), np.bincount(y_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([262, 261, 244, 290, 272, 220, 235, 269, 208, 239]),\n",
              " array([256, 268, 262, 271, 266, 232, 231, 276, 206, 232]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh9Pf_CyczM2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9b6e706-4e4e-4074-a3eb-31f1908a3291"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 92.48 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZSyPRIKdHeR",
        "colab_type": "text"
      },
      "source": [
        "Problem 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9rzttRAdHJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkMomentum():\n",
        "    def __init__(self,x,y):\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        self.delta_weights=[]\n",
        "        self.delta_bias=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr,beta=0.9,momentum=False):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(self.outputs[i])).reshape(len(self.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            if momentum:\n",
        "                self.delta_weights[i]=beta*self.delta_weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.delta_bias[i]=beta*self.delta_bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.weights[i]=self.weights[i]+self.delta_weights[i]\n",
        "                self.bias[i]=self.bias[i]+self.delta_bias[i]\n",
        "            else:\n",
        "                self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,beta,lr=1e-3,epoch=10):\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr,beta,momentum=True)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrLO-ruJdG-6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "fcdda71b-8f0a-4d97-f81a-484c69c8921b"
      },
      "source": [
        "neuron = NeuralNetworkMomentum(x_train, Labels)\n",
        "layer1 = Layer(100, 'sigmoid')\n",
        "layer2 = Layer(50, 'tanh')\n",
        "neuron.connect(x_train, layer1)\n",
        "neuron.connect(layer1, layer2)\n",
        "neuron.connect(layer2, Labels)\n",
        "neuron.train(batches=500, lr=0.1, epoch=20, beta=0.5)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.002017783444407748\n",
            "1 0.0012781967915228708\n",
            "2 0.0012555595624274309\n",
            "3 0.0011586001983104682\n",
            "4 0.00037248889935154644\n",
            "5 0.00028085807989264777\n",
            "6 7.78206282498589e-05\n",
            "7 5.853374177009288e-05\n",
            "8 4.990565018824331e-05\n",
            "9 0.0001378387654306032\n",
            "10 0.00010703481169217598\n",
            "11 1.3948708837239218e-05\n",
            "12 4.2646829030458464e-05\n",
            "13 4.089017302283832e-05\n",
            "14 5.529278689862014e-06\n",
            "15 6.611416310459818e-06\n",
            "16 4.3141944244476665e-06\n",
            "17 1.0121940113461146e-05\n",
            "18 1.0059310850684675e-05\n",
            "19 8.969774898630038e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y26-WaXhdGzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "479489d9-fba1-481a-a37d-08315c423f4d"
      },
      "source": [
        "pred = neuron.predict(x_test)\n",
        "np.bincount(neuron.predict(x_test)), np.bincount(y_test)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([260, 266, 256, 276, 264, 223, 235, 268, 214, 238]),\n",
              " array([256, 268, 262, 271, 266, 232, 231, 276, 206, 232]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHPqZJzid4gy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc87f4ad-413b-4002-b148-08b9d64cfebe"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 93.36 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}